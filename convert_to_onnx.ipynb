{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziro/anaconda3/envs/onnx_runtime_environment/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "import torch\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification, ORTModelForCustomTasks\n",
    "\n",
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths where original transformer models are downloaded\n",
    "model_id_bert = 'saved_models/bert-base-multilingual-uncased-sentiment'\n",
    "model_id_sent_transformer = 'saved_models/all-MiniLM-L6-v2'\n",
    "\n",
    "# Paths where ONNX models will be stored\n",
    "model_bert_path_onnx = 'saved_models/bert-base-multilingual-uncased-sentiment-onnx'\n",
    "model_sent_transformer_path_onnx = 'saved_models/all-MiniLM-L6-v2-onnx'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Runtime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before optimizing our models, we need to convert transformer model to onnx format. So, we will use `ORTModelForSequenceClassification` for bert model and `ORTModelForCustomTasks` for sentence transformer model. And we can use native  - `AutoTokenizer` from ðŸ¤—  library.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion of BERT Transformer Model to BERT ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('saved_models/bert-base-multilingual-uncased-sentiment-onnx/tokenizer_config.json',\n",
       " 'saved_models/bert-base-multilingual-uncased-sentiment-onnx/special_tokens_map.json',\n",
       " 'saved_models/bert-base-multilingual-uncased-sentiment-onnx/vocab.txt',\n",
       " 'saved_models/bert-base-multilingual-uncased-sentiment-onnx/added_tokens.json',\n",
       " 'saved_models/bert-base-multilingual-uncased-sentiment-onnx/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ORTModelForSequenceClassification.from_pretrained(model_id_bert, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id_bert)\n",
    "\n",
    "model.save_pretrained(model_bert_path_onnx)\n",
    "tokenizer.save_pretrained(model_bert_path_onnx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion of Sentence Transformer Model to Sentence Transformer ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('saved_models/all-MiniLM-L6-v2-onnx/tokenizer_config.json',\n",
       " 'saved_models/all-MiniLM-L6-v2-onnx/special_tokens_map.json',\n",
       " 'saved_models/all-MiniLM-L6-v2-onnx/vocab.txt',\n",
       " 'saved_models/all-MiniLM-L6-v2-onnx/added_tokens.json',\n",
       " 'saved_models/all-MiniLM-L6-v2-onnx/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ORTModelForCustomTasks.from_pretrained(model_id_sent_transformer, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id_sent_transformer)\n",
    "\n",
    "model.save_pretrained(model_sent_transformer_path_onnx)\n",
    "tokenizer.save_pretrained(model_sent_transformer_path_onnx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test these ONNX models. We will use our custom functions `src` folder rather than using `pipeline` from optimum library to ensure even at code level, ONNX models are compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from optimum.pipelines import pipeline\n",
    "\n",
    "# Original Transformer Model\n",
    "model_bert = AutoModelForSequenceClassification.from_pretrained(model_id_bert)\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(model_id_bert)\n",
    "\n",
    "# ONNX Model\n",
    "model_bert_onnx = ORTModelForSequenceClassification.from_pretrained(model_bert_path_onnx, file_name='model.onnx')\n",
    "tokenizer_bert_onnx = AutoTokenizer.from_pretrained(model_bert_path_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bertBaseMultiClass import sentiment_analyzer\n",
    "\n",
    "test_sentence = 'Whole Cake Island is the sweetest country to reside. But it will also spoils person due to comfort zones.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'1 star': 0.0116}, {'2 star': 0.0524}, {'3 star': 0.2946}, {'4 star': 0.4845}, {'5 star': 0.1569}]\n",
      "[{'1 star': 0.0116}, {'2 star': 0.0524}, {'3 star': 0.2946}, {'4 star': 0.4845}, {'5 star': 0.1569}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_analyzer(test_sentence, model_bert, tokenizer_bert))\n",
    "print(sentiment_analyzer(test_sentence, model_bert_onnx, tokenizer_bert_onnx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Transformer Model\n",
    "model_sent_transformer = AutoModel.from_pretrained(model_id_sent_transformer)\n",
    "tokenizer_sent_transformer = AutoTokenizer.from_pretrained(model_id_sent_transformer)\n",
    "\n",
    "# ONNX Model\n",
    "model_sent_transformer_onnx = ORTModelForCustomTasks.from_pretrained(model_sent_transformer_path_onnx, file_name='model.onnx')\n",
    "tokenizer_sent_transformer_onnx = AutoTokenizer.from_pretrained(model_sent_transformer_path_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sentencesSmilarity import sentences_similarity\n",
    "\n",
    "test_input = [\"This is an example of sentence.\", \"This is example of another sentence.\", \"Where is my book?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'This is example of another sentence.': 0.929}, {'Where is my book?': 0.03}]\n",
      "[{'This is example of another sentence.': 0.929}, {'Where is my book?': 0.03}]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_similarity(test_input, model_sent_transformer, tokenizer_sent_transformer))\n",
    "print(sentences_similarity(test_input, model_sent_transformer_onnx, tokenizer_sent_transformer_onnx))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that ONNX models are working successfully as these models are giving same output as original transformer models. Now, lets attempt to optimize these models further to accelerate latency and inferencing. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes `ORTOptimizer` will be used for graph optimization and `OptimizationConfig` takes in the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading ONNX models\n",
    "model_bert_onnx = ORTModelForSequenceClassification.from_pretrained(model_bert_path_onnx, file_name=\"model.onnx\")\n",
    "model_sent_transformer_onnx = ORTModelForCustomTasks.from_pretrained(model_sent_transformer_path_onnx, file_name='model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_onnx_optim_model(model, model_path):\n",
    "    '''\n",
    "        It allows further optimization of graphs for improving inferencing speed.\n",
    "    '''\n",
    "    optimizer = ORTOptimizer.from_pretrained(model_path)\n",
    "    optim_config = OptimizationConfig(optimization_level=99) # 1, 2 or 99\n",
    "    optimizer.optimize(optim_config, save_dir=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-04 08:33:42.367572830 [W:onnxruntime:, inference_session.cc:1458 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\n"
     ]
    }
   ],
   "source": [
    "save_onnx_optim_model(model_bert_onnx, model_bert_path_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in saved_models/all-MiniLM-L6-v2-onnx. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, blenderbot, blenderbot-small, bloom, camembert, canine, clip, codegen, convbert, convnext, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deit, detr, distilbert, donut-swin, dpr, dpt, electra, encoder-decoder, ernie, flaubert, flava, fnet, fsmt, funnel, glpn, gpt2, gpt_neo, gpt_neox, gptj, groupvit, hubert, ibert, imagegpt, layoutlm, layoutlmv2, layoutlmv3, led, levit, longformer, longt5, luke, lxmert, m2m_100, marian, maskformer, mbart, mctct, megatron-bert, mobilebert, mobilevit, mpnet, mt5, mvp, nezha, nystromformer, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roformer, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, splinter, squeezebert, swin, swinv2, t5, tapas, trajectory_transformer, transfo-xl, trocr, unispeech, unispeech-sat, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_mae, wav2vec2, wav2vec2-conformer, wavlm, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, yolos, yoso, onnx_model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m save_onnx_optim_model(model_sent_transformer_onnx, model_sent_transformer_path_onnx)\n",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m, in \u001b[0;36msave_onnx_optim_model\u001b[0;34m(model, model_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_onnx_optim_model\u001b[39m(model, model_path):\n\u001b[1;32m      2\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m        It allows further optimization of graphs for improving inferencing speed.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     optimizer \u001b[39m=\u001b[39m ORTOptimizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_path)\n\u001b[1;32m      6\u001b[0m     optim_config \u001b[39m=\u001b[39m OptimizationConfig(optimization_level\u001b[39m=\u001b[39m\u001b[39m99\u001b[39m) \u001b[39m# 1, 2 or 99\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[39m.\u001b[39moptimize(optim_config, save_dir\u001b[39m=\u001b[39mmodel_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/onnx_runtime_environment/lib/python3.10/site-packages/optimum/onnxruntime/optimization.py:84\u001b[0m, in \u001b[0;36mORTOptimizer.from_pretrained\u001b[0;34m(cls, model_or_path, file_names)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m CONFIG_NAME \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(model_or_path):\n\u001b[1;32m     83\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe local directory does not contain the configuration file \u001b[39m\u001b[39m{\u001b[39;00mCONFIG_NAME\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(model_or_path)\n\u001b[1;32m     85\u001b[0m onnx_model_path \u001b[39m=\u001b[39m []\n\u001b[1;32m     86\u001b[0m \u001b[39mfor\u001b[39;00m file_name \u001b[39min\u001b[39;00m file_names:\n",
      "File \u001b[0;32m~/anaconda3/envs/onnx_runtime_environment/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:775\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[39mif\u001b[39;00m pattern \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(pretrained_model_name_or_path):\n\u001b[1;32m    773\u001b[0m             \u001b[39mreturn\u001b[39;00m CONFIG_MAPPING[pattern]\u001b[39m.\u001b[39mfrom_dict(config_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwargs)\n\u001b[0;32m--> 775\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    776\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized model in \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    777\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShould have a `model_type` key in its \u001b[39m\u001b[39m{\u001b[39;00mCONFIG_NAME\u001b[39m}\u001b[39;00m\u001b[39m, or contain one of the following strings \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    778\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min its name: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(CONFIG_MAPPING\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    779\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized model in saved_models/all-MiniLM-L6-v2-onnx. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, blenderbot, blenderbot-small, bloom, camembert, canine, clip, codegen, convbert, convnext, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deit, detr, distilbert, donut-swin, dpr, dpt, electra, encoder-decoder, ernie, flaubert, flava, fnet, fsmt, funnel, glpn, gpt2, gpt_neo, gpt_neox, gptj, groupvit, hubert, ibert, imagegpt, layoutlm, layoutlmv2, layoutlmv3, led, levit, longformer, longt5, luke, lxmert, m2m_100, marian, maskformer, mbart, mctct, megatron-bert, mobilebert, mobilevit, mpnet, mt5, mvp, nezha, nystromformer, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roformer, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2, splinter, squeezebert, swin, swinv2, t5, tapas, trajectory_transformer, transfo-xl, trocr, unispeech, unispeech-sat, van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_mae, wav2vec2, wav2vec2-conformer, wavlm, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, yolos, yoso, onnx_model"
     ]
    }
   ],
   "source": [
    "save_onnx_optim_model(model_sent_transformer_onnx, model_sent_transformer_path_onnx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that still sentence transformers are still not supported for optimization. So, we will continue to quantize BERT model after comparing score of ONNX bert model after optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bert_onnx_opt = ORTModelForSequenceClassification.from_pretrained(model_bert_path_onnx, file_name=\"model_optimized.onnx\")\n",
    "tokenizer_bert_onnx = AutoTokenizer.from_pretrained(model_bert_path_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'1 star': 0.0116}, {'2 star': 0.0524}, {'3 star': 0.2946}, {'4 star': 0.4845}, {'5 star': 0.1569}]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = 'Whole Cake Island is the sweetest country to reside. But it will also spoils person due to comfort zones.'\n",
    "print(sentiment_analyzer(test_sentence, model_bert_onnx_opt, tokenizer_bert_onnx))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the scores are same before and after optimization of the models. Lets compare the size of original onnx model and optimized onnx model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Onnx Model file size: 638.68 MB\n",
      "Optimized Onnx Model file size: 638.46 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "onnx_path = Path(model_bert_path_onnx)\n",
    "\n",
    "# get model file size\n",
    "size = os.path.getsize(onnx_path / \"model.onnx\")/(1024*1024)\n",
    "print(f\"Original Onnx Model file size: {size:.2f} MB\")\n",
    "size = os.path.getsize(onnx_path / \"model_optimized.onnx\")/(1024*1024)\n",
    "print(f\"Optimized Onnx Model file size: {size:.2f} MB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not much difference of model sizes after optimization.\n",
    "\n",
    "Quantization allows for further accelaration of latency and inferencing. Lets see if the size of the model also decreases or not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_onnx_quantize_model(model, model_path):\n",
    "    quantizer = ORTQuantizer.from_pretrained(model)\n",
    "    qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)\n",
    "    quantizer.quantize(save_dir=model_path, quantization_config=qconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_onnx_quantize_model(model_bert_onnx, model_bert_path_onnx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are quantizing the original onnx model not optimized one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quantized = ORTModelForSequenceClassification.from_pretrained(model_bert_path_onnx, file_name=\"model_quantized.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'1 star': 0.249}, {'2 star': 0.1131}, {'3 star': 0.166}, {'4 star': 0.2143}, {'5 star': 0.2576}]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = 'Whole Cake Island is the sweetest country to reside. But it will also spoils person due to comfort zones.'\n",
    "print(sentiment_analyzer(test_sentence, model_quantized, tokenizer_bert_onnx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Onnx Model file size: 638.68 MB\n",
      "Optimized Onnx Model file size: 638.46 MB\n",
      "Quantized Onnx Model file size: 394.59 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "onnx_path = Path(model_bert_path_onnx)\n",
    "\n",
    "# get model file size\n",
    "size = os.path.getsize(onnx_path / \"model.onnx\")/(1024*1024)\n",
    "print(f\"Original Onnx Model file size: {size:.2f} MB\")\n",
    "size = os.path.getsize(onnx_path / \"model_optimized.onnx\")/(1024*1024)\n",
    "print(f\"Optimized Onnx Model file size: {size:.2f} MB\")\n",
    "size = os.path.getsize(onnx_path / \"model_quantized.onnx\")/(1024*1024)\n",
    "print(f\"Quantized Onnx Model file size: {size:.2f} MB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is significant reduction in size after quantization. \n",
    "- But the results are affected. Therefore, we cannot use it for production purpose.\n",
    "- We can use the optimized model as our final model for inferencing in the production environment. \n",
    "\n",
    "The above results may vary with model to model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnx_runtime_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "608916d3f599ad9539d25308a071842b8cbded10edbe257cdd255abc8f80b14d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
