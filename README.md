# Conversion of Transformer Models to ONNX (Open Neural Network eXchange) with ðŸ¤— Optimum 

## Brief Introduction

Inferencing using the models in various frameworks such Tensorflow, PyTorch and Hugging Face can be slow and inefficient. Therefore, these models needs to be first converted into a serialized format that can be loaded, optimized and executed on specialized runtime environments before deploying them. The model in this serialized format is known as ONNX. 

ONNX is general standard for representing Machine/Deep Learning models that can be used for variety of frameworks such as PyTorch, Tensorflow and Hugging Face. We will be using open source library, ðŸ¤— Optimum, for achieving maximum efficiency to train and run models. 

In this tutorial, we will not further deep dive into technical aspects of ONNX. Main aim of this project is to get started with conversion of transformer models to ONNX models. We will see how ðŸ¤— Transformer will be converted into ONNX model and then we will see how we will do inferencing. Note that inferencing is done on CPU as PyTorch GPU is not included in `requirements.txt` file.

## Setting up Environment

1. Clone the entire repository. 
```bash
git clone https://github.com/di37/convert-transformers-to-onnx.git
```

2. Create conda environment. 
```bash
conda create -n onnx_runtime_environment python=3.10
```

3. Activate the environment.
```bash
conda activate onnx_runtime_environment
```

4. Install the dependencies.
```bash
pip install -r requirements.txt --no-cache-dir
```

5. For fair comparison between the speed of original transformer model and after being converted into ONNX format, we will be downloading 
the transformer models in our local drive. The below command will allow to download these models in `saved_models` folder.
```bash
python download_models.py
```
Ensure that the models are stored in following file structure.

```bash
saved_models
â”œâ”€â”€ all-MiniLM-L6-v2
â”‚Â Â  â”œâ”€â”€ 1_Pooling
â”‚Â Â  â”‚Â Â  â””â”€â”€ config.json
â”‚Â Â  â”œâ”€â”€ 2_Normalize
â”‚Â Â  â”œâ”€â”€ config.json
â”‚Â Â  â”œâ”€â”€ config_sentence_transformers.json
â”‚Â Â  â”œâ”€â”€ modules.json
â”‚Â Â  â”œâ”€â”€ pytorch_model.bin
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â”œâ”€â”€ sentence_bert_config.json
â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
â”‚Â Â  â”œâ”€â”€ tokenizer.json
â”‚Â Â  â””â”€â”€ vocab.txt
â””â”€â”€bert-base-multilingual-uncased-sentiment
 Â Â  â”œâ”€â”€ config.json
 Â Â  â”œâ”€â”€ pytorch_model.bin
 Â Â  â”œâ”€â”€ special_tokens_map.json
 Â Â  â”œâ”€â”€ tokenizer_config.json
 Â Â  â”œâ”€â”€ tokenizer.json
 Â Â  â””â”€â”€ vocab.txt
```

6. All of the details for conversion of onnx models are included in `convert_to_onnx.ipynb`.

## References
1. Convert Transformers to ONNX with Hugging Face Optimum - https://huggingface.co/blog/convert-transformers-to-onnx
2. Accelerated Inference with Optimum and Transformers Pipelines - https://huggingface.co/blog/optimum-inference
3. Accelerate Transformer inference on CPU with Optimum and ONNX - https://www.youtube.com/watch?v=_AKFDOnrZz8